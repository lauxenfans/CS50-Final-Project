{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lauxenfans/CS50-Final-Project/blob/main/sec01/cs1090b_sec01_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd284aca",
      "metadata": {
        "id": "cd284aca"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Harvard-CS1090/2026_CS1090B_public/blob/main/sec01/cs1090b_sec01_solutions.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-0",
      "metadata": {
        "id": "cell-0"
      },
      "source": [
        "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS1090B Introduction to Data Science\n",
        "\n",
        "## Section 1:  Anatomy of a Neural Network, NNs with PyTorch\n",
        "**Harvard University**<br/>\n",
        "**Spring 2026**<br/>\n",
        "**Instructors**: Pavlos Protopapas, Kevin Rader, and Chris Gumb<br/>\n",
        "<br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-setup-0",
      "metadata": {
        "id": "cell-setup-0"
      },
      "source": [
        "## Setup: Download Data\n",
        "\n",
        "Run this cell first to download the required data files from GitHub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-setup-1",
      "metadata": {
        "id": "cell-setup-1"
      },
      "outputs": [],
      "source": [
        "# Environment detection and setup\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "import shutil\n",
        "\n",
        "# Define the zip file URL and expected directories\n",
        "assets_zip_url = \"https://github.com/Harvard-CS1090/2026_CS1090B_public/raw/main/sec01/notebook_assets.zip\"\n",
        "\n",
        "assets_zip_name = \"notebook_assets.zip\"\n",
        "expected_dirs = [\"data\", \"fig\"]\n",
        "\n",
        "# Check if required directories already exist\n",
        "all_dirs_exist = all(os.path.isdir(d) for d in expected_dirs)\n",
        "\n",
        "if all_dirs_exist:\n",
        "    print(\"Required directories already exist. Skipping download.\")\n",
        "else:\n",
        "    print(f\"Downloading {assets_zip_name} from GitHub...\")\n",
        "\n",
        "    # Use wget in Colab, or urllib for local\n",
        "    try:\n",
        "        if 'google.colab' in sys.modules:\n",
        "            subprocess.run(['wget', '-q', assets_zip_url], check=True)\n",
        "        else:\n",
        "            import urllib.request\n",
        "            urllib.request.urlretrieve(assets_zip_url, assets_zip_name)\n",
        "        print(f\"Downloaded {assets_zip_name}.\")\n",
        "\n",
        "        # Unzip the file\n",
        "        import zipfile\n",
        "        with zipfile.ZipFile(assets_zip_name, 'r') as zip_ref:\n",
        "            zip_ref.extractall('.')\n",
        "        print(f\"Extracted {assets_zip_name}.\")\n",
        "\n",
        "        # Clean up the zip file\n",
        "        os.remove(assets_zip_name)\n",
        "        print(f\"Removed {assets_zip_name}.\")\n",
        "\n",
        "        # Remove __MACOSX folder if it exists\n",
        "        if os.path.isdir('__MACOSX'):\n",
        "            shutil.rmtree('__MACOSX')\n",
        "            print(\"Removed __MACOSX folder.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during setup: {e}\", file=sys.stderr)\n",
        "\n",
        "print(\"Setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-2",
      "metadata": {
        "id": "cell-2"
      },
      "outputs": [],
      "source": [
        "# Standard library\n",
        "from collections import Counter\n",
        "\n",
        "# Data science\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-3",
      "metadata": {
        "id": "cell-3"
      },
      "source": [
        "## Part 1:  Anatomy of a Neural Network\n",
        "\n",
        "To learn about the components of a neural network, we'll attempt to model the following data as a binary classification problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-6",
      "metadata": {
        "id": "cell-6"
      },
      "outputs": [],
      "source": [
        "# Load data from CSV\n",
        "df = pd.read_csv('data/toydata.csv')\n",
        "x = df.x.values\n",
        "y = df.y.values\n",
        "print(df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-7",
      "metadata": {
        "id": "cell-7"
      },
      "outputs": [],
      "source": [
        "plt.scatter(x, y)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y (1 or 0)')\n",
        "plt.title(\"Toy Binary Classification Data\");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5791x3gu967",
      "metadata": {
        "id": "5791x3gu967"
      },
      "source": [
        "Our modeling assumptions are:\n",
        "\n",
        "$Y \\sim Bern(p)$  `(Y follows a Bernoulli distribution with parameter p)`\n",
        "\n",
        "and\n",
        "\n",
        "$p = f(x)$\n",
        "\n",
        "That is, the probability that $Y=1$ is a function of $x$.\n",
        "\n",
        "---\n",
        "\n",
        "### Neural Networks as a Composition of Functions\n",
        "\n",
        "So how do we learn this function $f$? A neural network is a composition of many simple functions. In this example our basic building block will be the **rectified linear unit**, or **ReLU**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-11",
      "metadata": {
        "id": "cell-11"
      },
      "outputs": [],
      "source": [
        "def relu(z: np.ndarray) -> np.ndarray :\n",
        "    return  np.maximum(0, z)\n",
        "# or\n",
        "# relu = lambda z: np.maximum(0, z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-12",
      "metadata": {
        "id": "cell-12"
      },
      "outputs": [],
      "source": [
        "# Create an input vector of dummy x values\n",
        "x_lin = np.linspace(-3,3,300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-13",
      "metadata": {
        "id": "cell-13"
      },
      "outputs": [],
      "source": [
        "# Plot the ReLU function\n",
        "plt.figure(figsize=(2,2))\n",
        "plt.plot(x_lin, relu(x_lin), c='r')\n",
        "plt.title('ReLU');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-14",
      "metadata": {
        "id": "cell-14"
      },
      "outputs": [],
      "source": [
        "# With data scatter plot\n",
        "plt.plot(x_lin, relu(x_lin), c='r', label='ReLU')\n",
        "plt.scatter(x, y, label='data')\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-15",
      "metadata": {
        "id": "cell-15"
      },
      "source": [
        "### Affine Transformation: weights & biases\n",
        "\n",
        "To better fit the data, we could try scaling and shifting the inputs to relu.\n",
        "\n",
        "We call the scaling factor the **weight**, $w$, and the amount of the shift the **bias**, $b$.  \n",
        "The bias term allows the decision boundary to shift away from the origin.\n",
        "Without a bias, the model would be forced to make predictions that pass through (0, 0), which is rarely appropriate for real data.\n",
        "\n",
        "The scaling and shifting our input is called an **affine transformation**. We refer to the output of the affine transformation as $z$.\n",
        "\n",
        "> **‚ùìQuestion 1:** What is the difference between an *affine transformation* and a *linear transformation*?\n",
        "\n",
        "<details>\n",
        "<summary><em>Click to reveal answer</em></summary>\n",
        "\n",
        "> A **linear transformation** ($z = w \\cdot x$) strictly scales the input and must pass through the origin $(0,0)$.\n",
        ">\n",
        "> An **affine transformation** ($z = w \\cdot x + b$) adds a **bias** term ($b$) that shifts the result, allowing the model's decision boundary to offset from the origin.\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-18",
      "metadata": {
        "id": "cell-18"
      },
      "source": [
        "### Activation Function\n",
        "\n",
        "Applying the non-linear function (relu) to the affine transformation is called **activation**. Without a nonlinear activation function, stacking multiple layers would still\n",
        "result in a linear model, no matter how many layers we add.\n",
        "\n",
        "Relu is our **activation function**! We often refer to the output of the activation as $h$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-21",
      "metadata": {
        "id": "cell-21"
      },
      "source": [
        "### One Lonely Neuron\n",
        "\n",
        "The affine transformation combined with the activation makes up a single **neuron** (sometimes called a 'unit').\n",
        "\n",
        "(Note: We will use the terms neuron, node, and unit interchangeably.  They all refer to the same computational element.)\n",
        "\n",
        "<img src='https://github.com/Harvard-CS1090/2026_CS1090B_public/blob/main/sec01/fig/nn1.png?raw=1' width=400>\n",
        "\n",
        "\n",
        "*(Note: The diagram uses statistical notation from lecture. Here's how it maps to our code:)*\n",
        "- *Œ≤‚ÇÅ ‚Üí `w` (weight)*\n",
        "- *Œ≤‚ÇÄ ‚Üí `b` (bias)*\n",
        "- *h in diagram ‚Üí `z` in code (affine output, before activation)*\n",
        "- *y in diagram ‚Üí `h` in code (activation output)*\n",
        "- *The diagram shows sigmoid; we use ReLU*\n",
        "\n",
        "Try shifting to the left by 2, Try reversing the direction and shifting to the *right* by 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-25",
      "metadata": {
        "id": "cell-25"
      },
      "outputs": [],
      "source": [
        "w = -1\n",
        "b = 2\n",
        "z = w*x_lin + b\n",
        "h = relu(z)\n",
        "plt.plot(x_lin, h, c='r', label='neuron output')\n",
        "plt.scatter(x, y, label='toy data')\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nv2t4lbi7zm",
      "metadata": {
        "id": "nv2t4lbi7zm"
      },
      "source": [
        "> **‚ùìQuestion 2: Predict the Output**\n",
        ">\n",
        "> If we change the weights and bias to `w=2, b=-1` instead of `w=-1, b=2`:\n",
        ">\n",
        "> 1. In which direction will the ReLU \"activate\" (for positive x or negative x)?\n",
        "> 2. At what x-value will the neuron start outputting non-zero values?\n",
        "> 3. Will the output increase or decrease as x increases beyond that point?\n",
        ">\n",
        "> **Try it:** Modify the code in the cell above to test your prediction!\n",
        "\n",
        "<details>\n",
        "<summary><em>Click to reveal answer</em></summary>\n",
        "\n",
        "> 1. Direction: The ReLU will activate for positive x (since the slope $w=2$ is positive).\n",
        "> 2. Start Point: The neuron starts outputting non-zero values at $x = 0.5$ (solving $2x - 1 = 0$).\n",
        "> 3. Trend: The output will increase as x increases (slope is positive).\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-26",
      "metadata": {
        "id": "cell-26"
      },
      "source": [
        "### Multiple Layers: hidden & output\n",
        "\n",
        "We saw that we could make two different neurons, each of which fits only *half* of the data well. Can we *combine* them in some way to get better predictions??\n",
        "\n",
        "Let's try adding them.\n",
        "\n",
        "<img src='https://github.com/Harvard-CS1090/2026_CS1090B_public/blob/main/sec01/fig/nn2.png?raw=1' width=500>\n",
        "\n",
        "*(Note: The diagram uses different notation:)*\n",
        "- *z‚ÇÅ, z‚ÇÇ in diagram ‚Üí `h1`, `h2` in code (activation outputs)*\n",
        "- *œÉ in diagram ‚Üí we use ReLU*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-29",
      "metadata": {
        "id": "cell-29"
      },
      "outputs": [],
      "source": [
        "w1, b1 = -1, 2\n",
        "w2, b2 = 1, 2\n",
        "z1 = w1*x_lin + b1\n",
        "z2 = w2*x_lin + b2\n",
        "h1 = relu(z1)\n",
        "h2 = relu(z2)\n",
        "plt.plot(x_lin, h1+h2)\n",
        "plt.scatter(x, y)\n",
        "plt.title(\"Oops!\");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-30",
      "metadata": {
        "id": "cell-30"
      },
      "source": [
        "We're obviously not there yet! It looks like we might want to scale and shift $h_1$ & $h_2$ again.\n",
        "\n",
        "In fact, $h_1$ and $h_2$ will become inputs to a *new* **layer**, the **output layer**. Our output layer will give a single neuron because our output is a scalar (the probability of $y=1$).\n",
        "\n",
        "$h_1$ & $h_2$ are neurons in the **hidden layer**. \"Hidden\" in the sense that we don't actually observe their outputs; they are passed to the next layer as input. A NN can have *many* hidden layers.\n",
        "\n",
        "The output neuron is the **output layer** (you can also have multiple output layers, but that is a story for another time!)\n",
        "\n",
        "The output neuron will need a weight & bias for *both* $h_1$ and $h_2$.\n",
        "\n",
        "It will also need its own **output activation function**!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-35",
      "metadata": {
        "id": "cell-35"
      },
      "source": [
        "### Output Activation\n",
        "\n",
        "<img src='https://github.com/Harvard-CS1090/2026_CS1090B_public/blob/main/sec01/fig/nn3.png?raw=1' width=600>\n",
        "\n",
        "*(Note: The diagram uses different notation:)*\n",
        "- *z‚ÇÅ, z‚ÇÇ in diagram ‚Üí `h1`, `h2` in code (hidden layer outputs)*\n",
        "- *W‚ÇÉ‚ÇÅ, W‚ÇÉ‚ÇÇ, W‚ÇÉ‚ÇÄ ‚Üí `w31`, `w32`, `b3` in code*\n",
        "- *q in diagram ‚Üí `z3` in code (output layer affine result)*\n",
        "- *œÉ in hidden layers ‚Üí we use ReLU; output layer uses sigmoid*\n",
        "\n",
        "Because we are modeling a binary $y$, we want our output to represent the probability of $y=1$. This means that the output must be in [0,1].\n",
        "\n",
        "But relu is $[0, \\infty]$! We need a different activation function for our output neuron.\n",
        "\n",
        "What function do you know that has output bounded in [0,1]?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-39",
      "metadata": {
        "id": "cell-39"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z: np.ndarray) -> np.ndarray :\n",
        "    return 1/(1 + np.exp(-z))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-40",
      "metadata": {
        "id": "cell-40"
      },
      "outputs": [],
      "source": [
        "# plot sigmoid activation\n",
        "plt.figure(figsize=(2,2))\n",
        "plt.plot(x_lin, sigmoid(x_lin))\n",
        "plt.title('Sigmoid');"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-41",
      "metadata": {
        "id": "cell-41"
      },
      "source": [
        "### Constructing our First MLP (Multilayer Perceptron aka a Neural Network)\n",
        "\n",
        "Now all we need to do is define weights & bias for all three neurons.\n",
        "**Remember!** The output neuron needs a weight for *each* of its inputs ($h_1$ & $h_2$)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-43",
      "metadata": {
        "id": "cell-43"
      },
      "outputs": [],
      "source": [
        "# hidden layer\n",
        "w1, b1 = 6, -5\n",
        "w2, b2 = -6, -5\n",
        "z1 = w1*x_lin + b1\n",
        "z2 = w2*x_lin + b2\n",
        "h1 = relu(z1)\n",
        "h2 = relu(z2)\n",
        "# output layer\n",
        "w31, w32, b3 = -2,-2,4\n",
        "z3 = w31*h1 + w32*h2 + b3\n",
        "output = sigmoid(z3)\n",
        "\n",
        "# plot predictions\n",
        "plt.plot(x_lin, output, c='r')\n",
        "plt.scatter(x, y)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('P(Y=1|x)');"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-44",
      "metadata": {
        "id": "cell-44"
      },
      "source": [
        "Now let us calculate the accuracy of our predictions!\n",
        "\n",
        "Bear in mind, that what we got so far is not the prediction yet, but rather just probabilities. If a probability is above 0.5, then our prediction is 1, else our prediction is 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-45",
      "metadata": {
        "id": "cell-45"
      },
      "outputs": [],
      "source": [
        "# hidden layer\n",
        "z1 = w1*x + b1\n",
        "z2 = w2*x + b2\n",
        "h1 = relu(z1)\n",
        "h2 = relu(z2)\n",
        "# output layer\n",
        "z3 = w31*h1 + w32*h2 + b3\n",
        "p = sigmoid(z3)\n",
        "\n",
        "# output values > 0.5 are predicted to be 1, otherwise 0\n",
        "y_hat  = (p > 0.5).astype(int)\n",
        "# calculate the accuracies\n",
        "acc = sum((y_hat == y)/len(y))\n",
        "print(f\"Accuracy: {acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-48",
      "metadata": {
        "id": "cell-48"
      },
      "source": [
        "## Part 2: NN with PyTorch\n",
        "\n",
        "\n",
        "**[PyTorch nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)**\n",
        "\n",
        "\n",
        "\n",
        "Now that we understand what a neural network computes conceptually, we will implement the same ideas using PyTorch.\n",
        "\n",
        "There are many powerful deep learning packages to work with neural networks like **TensorFlow** and **PyTorch**. These packages provide both the forward and backward propagations, and many other functionalities. The forward pass is used to make predictions while the backward (backpropagation)  is used to  train (optimize) a network. 'Training' refers to find the optimal parameters for a specific task. We'll talk more about optimization and backpropagation next week!\n",
        "\n",
        "PyTorch provides a simple way to build neural networks using `nn.Sequential` which allows us to stack layers in order.\n",
        "\n",
        "Here, we use PyTorch's `nn.Sequential` to create a neural network for our toy binary classification problem.\\\n",
        "We'll use the same 'architecture' as above:\n",
        "- 1 hidden layer with 2 neurons using ReLU activation\n",
        "- an output layer with 1 neuron using sigmoid activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-50",
      "metadata": {
        "id": "cell-50"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# define a sequential model called 'first_model'\n",
        "model = nn.Sequential(\n",
        "    # add a hidden layer with 2 neurons and relu activations\n",
        "    # note we specify the input dimension is 1 (scalar)\n",
        "    nn.Linear(1, 2),\n",
        "    nn.ReLU(),\n",
        "    # add an output layer containing a single neuron with sigmoid activation\n",
        "    # your code here\n",
        "    # ...\n",
        ")\n",
        "\n",
        "# display model summary\n",
        "print(model)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-51",
      "metadata": {
        "id": "cell-51"
      },
      "source": [
        "### Setting up Training\n",
        "\n",
        "In PyTorch, we define the optimizer and loss function separately, then write a training loop.\n",
        "Unlike Keras, PyTorch requires us to write our own training loop, which gives us more control over the training process.\n",
        "\n",
        "`optimizer` - defines how the weights are updated (we'll use SGD; more on this next lecture!)\\\n",
        "`loss` - what the model is trying to minimize (criterion)\n",
        "\n",
        "\n",
        "<img src='https://github.com/Harvard-CS1090/2026_CS1090B_public/blob/main/sec01/fig/nn4.png?raw=1' width=600>\n",
        "\n",
        "\n",
        "\n",
        "> **‚ùì Question 3:** Why do I want to track metrics if I already have a loss?\n",
        "\n",
        "\n",
        "<details>\n",
        "<summary><em>Click to reveal answer</em></summary>\n",
        "\n",
        "> Loss (e.g., BCE) is a differentiable function designed for the optimizer to minimize.\n",
        "\n",
        "> Metrics (e.g., Accuracy) are interpretable measures of actual performance that we care about, even if they aren't differentiable.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-54",
      "metadata": {
        "id": "cell-54"
      },
      "outputs": [],
      "source": [
        "# Define loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-55",
      "metadata": {
        "id": "cell-55"
      },
      "source": [
        "### Fitting the NN\n",
        "\n",
        "In PyTorch, we write a training loop that:\n",
        "1. **Forward pass**: Compute predictions\n",
        "2. **Compute loss**: Compare predictions to targets\n",
        "3. **Backward pass**: Compute gradients via `loss.backward()`\n",
        "4. **Update weights**: Apply gradients via `optimizer.step()`\n",
        "\n",
        "**Training Loop**\n",
        "\n",
        "Training a neural network is an iterative process.  Each pass through the training loop makes a small adjustment to the model‚Äôs  parameters rather than learning everything at once.\n",
        "\n",
        "```\n",
        "   X_train --> Forward Pass  -->  Loss --> Backward --> Update Weights\n",
        "                model(X_train)    BCE      .backward()   optimizer.step()\n",
        "                ‚îÇ                                                  ‚îÇ\n",
        "                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ repeat for N epochs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "**Few Notes:**\n",
        "\n",
        "`batch_size` - number of observations over which the loss is calculated before each weight update (more on this next lecture)\\\n",
        "`epochs` - number of times the complete dataset is seen in the fitting process\n",
        "\n",
        "**A note on initialization:** Neural network weights are initialized randomly before training. PyTorch's `nn.Linear` uses Kaiming uniform initialization by default, which works well for ReLU networks. However, with  only 2 neurons in our hidden layer, the initial weight *signs* matter: if both weights start positive, both ReLUs \"turn off\" for negative x values, and the model may converge to a solution that can't predict P(y=1)‚âà0 on the left side of the data.\n",
        "\n",
        "We use **Xavier initialization** (`nn.init.xavier_uniform_`) which samples weights from a distribution scaled by the layer sizes. This helps maintain constant variance of activations across layers, improving training stability. The `torch.manual_seed(0)` is set before initialization for reproducibility - with this seed, Xavier produces weights with opposite signs in our first layer, ensuring one neuron stays active for each side of the x-axis. (More on this next week)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-57",
      "metadata": {
        "id": "cell-57"
      },
      "outputs": [],
      "source": [
        "# Prepare data as PyTorch tensors\n",
        "X_tensor = torch.FloatTensor(x.reshape(-1, 1))\n",
        "y_tensor = torch.FloatTensor(y.reshape(-1, 1))\n",
        "\n",
        "# Split into train and validation (80/20) with fixed seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "n_train = int(0.8 * len(X_tensor))\n",
        "indices = torch.randperm(len(X_tensor)) # Randomly permute the indices of the tensor\n",
        "train_idx, val_idx = indices[:n_train], indices[n_train:]\n",
        "\n",
        "X_train, X_val = X_tensor[train_idx], X_tensor[val_idx]\n",
        "y_train, y_val = y_tensor[train_idx], y_tensor[val_idx]\n",
        "\n",
        "# Reinitialize model for training (using different seed for better initialization)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(1, 2),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(2, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "# Apply Xavier initialization\n",
        "for layer in model:\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        nn.init.xavier_uniform_(layer.weight)\n",
        "        nn.init.zeros_(layer.bias)\n",
        "\n",
        "# Use SGD optimizer (same as TensorFlow version)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Training loop\n",
        "epochs = 400\n",
        "history = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': []}\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    optimizer.zero_grad() # Zero gradients Reset for next iteration\n",
        "\n",
        "    # Forward pass\n",
        "    y_pred = model(X_train)\n",
        "    loss = criterion(y_pred, y_train)\n",
        "\n",
        "    # Backward pass\n",
        "    # PyTorch automatically computes gradients using a system called autograd.\n",
        "    # During the forward pass, it records operations; during the backward pass,\n",
        "    # it applies the chain rule to compute gradients for each parameter.\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Calculate metrics (after weight update)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred_eval = model(X_train)\n",
        "        train_acc = ((y_pred_eval > 0.5).float() == y_train).float().mean().item()\n",
        "\n",
        "        # Validation\n",
        "        y_val_pred = model(X_val)\n",
        "        val_loss = criterion(y_val_pred, y_val)\n",
        "        val_acc = ((y_val_pred > 0.5).float() == y_val).float().mean().item()\n",
        "\n",
        "    # Store history\n",
        "    history['loss'].append(loss.item())\n",
        "    history['val_loss'].append(val_loss.item())\n",
        "    history['accuracy'].append(train_acc)\n",
        "    history['val_accuracy'].append(val_acc)\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch {epoch+1}/{epochs} - loss: {loss.item():.4f} - accuracy: {train_acc:.4f} - val_loss: {val_loss.item():.4f} - val_accuracy: {val_acc:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-58",
      "metadata": {
        "id": "cell-58"
      },
      "source": [
        "### Plot Training History\n",
        "We stored the training history in a dictionary. Use it to plot the loss across epochs. Don't forget those axis labels! We need to distinguish train from validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-59",
      "metadata": {
        "id": "cell-59"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Loss plot\n",
        "axes[0].plot(history['loss'], label='train')\n",
        "axes[0].plot(history['val_loss'], label='validation')\n",
        "axes[0].set_xlabel('epoch')\n",
        "axes[0].set_ylabel('binary cross-entropy loss')\n",
        "axes[0].set_title('Loss')\n",
        "axes[0].legend()\n",
        "\n",
        "# Accuracy plot\n",
        "axes[1].plot(history['accuracy'], label='train')\n",
        "axes[1].plot(history['val_accuracy'], label='validation')\n",
        "axes[1].set_xlabel('epoch')\n",
        "axes[1].set_ylabel('accuracy')\n",
        "axes[1].set_title('Accuracy')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-60",
      "metadata": {
        "id": "cell-60"
      },
      "outputs": [],
      "source": [
        "# Print final metrics\n",
        "print(f\"Final Training Loss: {history['loss'][-1]:.4f}\")\n",
        "print(f\"Final Validation Loss: {history['val_loss'][-1]:.4f}\")\n",
        "print(f\"Final Training Accuracy: {history['accuracy'][-1]:.4f}\")\n",
        "print(f\"Final Validation Accuracy: {history['val_accuracy'][-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xthppy5rhb",
      "metadata": {
        "id": "xthppy5rhb"
      },
      "source": [
        "> **‚ùìQuestion 4: Interpret the Training Curves**\n",
        ">\n",
        "> Look at the training and validation loss/accuracy curves above and answer:\n",
        ">\n",
        "> 1. Is the model **overfitting**, **underfitting**, or **well-fit**? How can you tell from the curves?\n",
        "> 2. If the validation loss started *increasing* while the training loss kept *decreasing*, what would that indicate?\n",
        "> 3. Based on these curves, would training for more epochs likely improve performance? Why or why not?\n",
        "\n",
        "\n",
        "<details>\n",
        "<summary><em>Click to reveal answer</em></summary>\n",
        "\n",
        "> 1. The model is well-fit. The validation loss is low (even lower than training loss) and accuracy is high, showing it generalizes well.\n",
        "> 2. If validation loss started increasing while training loss decreased, it would indicate overfitting.\n",
        "> 3. Unlikely - loss seems stable and low\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-61",
      "metadata": {
        "id": "cell-61"
      },
      "source": [
        "### Predict & Plot\n",
        "We use the model in evaluation mode on a linspace, `x_lin`, which we previously constructed to span the range of the dataset's $x$ values. We save the resulting predictions in `y_hat`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-62",
      "metadata": {
        "id": "cell-62"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    X_lin_tensor = torch.FloatTensor(x_lin.reshape(-1, 1))\n",
        "    y_hat = model(X_lin_tensor).numpy()\n",
        "\n",
        "plt.plot(x_lin, y_hat, c='r', label='predictions')\n",
        "plt.scatter(x, y, label='data')\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-63",
      "metadata": {
        "id": "cell-63"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the full dataset\n",
        "# (Note that in this example we did not have a separate test set!)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_full = model(X_tensor)\n",
        "    loss_full = criterion(y_pred_full, y_tensor)\n",
        "    acc_full = ((y_pred_full > 0.5).float() == y_tensor).float().mean()\n",
        "print(f'Loss: {loss_full.item():.4f}, Accuracy: {acc_full.item():.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-64",
      "metadata": {
        "id": "cell-64"
      },
      "outputs": [],
      "source": [
        "# View model weights & biases for all layers\n",
        "for name, param in model.named_parameters():\n",
        "    print(f'{name}: {param.data}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aiw7l2ds1al",
      "metadata": {
        "id": "aiw7l2ds1al"
      },
      "source": [
        "**Interpreting the weights and biases:**\n",
        "\n",
        "Based on the tensor shapes, our model has a **1 ‚Üí 2 ‚Üí 1** architecture:\n",
        "```\n",
        "Input (1) ‚Üí Linear(1,2) ‚Üí ReLU ‚Üí Linear(2,1) ‚Üí Sigmoid ‚Üí Output\n",
        "             layer 0      layer 1    layer 2     layer 3\n",
        "```\n",
        "\n",
        "**Layer 0 (input ‚Üí hidden):**\n",
        "- `0.weight` shape `(2, 1)`: Each row is a hidden neuron's weight for the single input\n",
        "- `0.bias` shape `(2,)`: One bias per hidden neuron\n",
        "\n",
        "Each hidden neuron computes: `activation = input √ó weight + bias`\n",
        "\n",
        "**Layer 2 (hidden ‚Üí output):**\n",
        "- `2.weight` shape `(1, 2)`: The output neuron's weights for each hidden neuron\n",
        "- `2.bias` shape `(1,)`: The output neuron's bias\n",
        "\n",
        "**Why are layers 1 and 3 missing?** ReLU and Sigmoid are activation functions with no learnable parameters, so they don't appear in `named_parameters()`.\n",
        "\n",
        "**Naming convention:** `{layer_index}.{parameter_type}` ‚Äî the index corresponds to the layer's position in the Sequential model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-65",
      "metadata": {
        "id": "cell-65"
      },
      "source": [
        "### Save & Reload model\n",
        "Fitting non-trivial neural networks can take a long time. It is important to know how to save and reload your PyTorch models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-66",
      "metadata": {
        "id": "cell-66"
      },
      "outputs": [],
      "source": [
        "# save model\n",
        "torch.save(model.state_dict(), 'first_pytorch_model.pth')\n",
        "\n",
        "# To reload, first recreate the model architecture\n",
        "model_reload = nn.Sequential(\n",
        "    nn.Linear(1, 2),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(2, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "# Then load the saved weights\n",
        "model_reload.load_state_dict(torch.load('first_pytorch_model.pth', weights_only=True))\n",
        "model_reload.eval()\n",
        "\n",
        "# display summary\n",
        "print(model_reload)\n",
        "\n",
        "# Confirm we can make predictions with loaded model\n",
        "with torch.no_grad():\n",
        "    y_hat = model_reload(X_lin_tensor).numpy()\n",
        "\n",
        "plt.plot(x_lin, y_hat, c='r', label='predictions')\n",
        "plt.scatter(x, y, label='data')\n",
        "plt.title(\"Loaded Model\")\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-68",
      "metadata": {
        "id": "cell-68"
      },
      "source": [
        "### An Ugly Function (Regression Example)\n",
        "\n",
        "Now let's try a regression task where the response variable we are modeling can take on any real value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-70",
      "metadata": {
        "id": "cell-70"
      },
      "outputs": [],
      "source": [
        "def ugly_function(x):\n",
        "    if x < 0:\n",
        "        return np.exp(-(x**2))/2 + 1 + np.exp(-((10*x)**2))\n",
        "    else:\n",
        "        return np.exp(-(x**2)) + np.exp(-((10*x)**2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-71",
      "metadata": {
        "id": "cell-71"
      },
      "source": [
        "How do you feel about the prospect of manually setting the weights to approximate this beauty?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-72",
      "metadata": {
        "id": "cell-72"
      },
      "outputs": [],
      "source": [
        "# Generate data\n",
        "x_ugly = np.linspace(-3,3,1500) # create x-values for input\n",
        "y_ugly = np.array(list(map(ugly_function, x_ugly)))\n",
        "\n",
        "# Plot data\n",
        "plt.plot(x_ugly, y_ugly);\n",
        "plt.title('An Ugly Function')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y');"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-73",
      "metadata": {
        "id": "cell-73"
      },
      "source": [
        "---\n",
        "\n",
        "## üèãÔ∏è TEAM ACTIVITY: Build Model\n",
        "\n",
        "We're Gonna Need a Bigger Model...\n",
        "\n",
        "---\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-74",
      "metadata": {
        "id": "cell-74"
      },
      "source": [
        "Construct an NN to approximate the ugly function with PyTorch's `nn.Sequential`.\n",
        "\n",
        "**Build Model & Display Summary**\n",
        "\n",
        "You can play with the number of hidden (`Linear`) layers and the number of neurons in each.\n",
        "\n",
        "Think about what should the output activation function be for a regression task?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-75",
      "metadata": {
        "id": "cell-75"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-76",
      "metadata": {
        "id": "cell-76"
      },
      "source": [
        "**Compile (Setup Optimizer & Loss)**\\\n",
        "Use the `SGD` optimizer and `MSELoss` as your loss.\\\n",
        "You can experiment with the `learning_rate`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-77",
      "metadata": {
        "id": "cell-77"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-78",
      "metadata": {
        "id": "cell-78"
      },
      "source": [
        "**Fit**\\\n",
        "Fit `ugly_model` on `x_ugly` and `y_ugly`.\\\n",
        "You can experiment with the number of `epochs`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-79",
      "metadata": {
        "id": "cell-79"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-80",
      "metadata": {
        "id": "cell-80"
      },
      "source": [
        "**Plot Training History**\n",
        "\n",
        "Plot the model's training history. Don't forget your axis labels!\\\n",
        "**Hint:** Remember, we stored the history in a dictionary during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-82",
      "metadata": {
        "id": "cell-82"
      },
      "outputs": [],
      "source": [
        "# Plot History\n",
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-83",
      "metadata": {
        "id": "cell-83"
      },
      "source": [
        "**Get Predictions**\\\n",
        "Use your model to predict on `x_ugly` and store the results in a variable called `y_hat_ugly`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-84",
      "metadata": {
        "id": "cell-84"
      },
      "outputs": [],
      "source": [
        "# Predict\n",
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-85",
      "metadata": {
        "id": "cell-85"
      },
      "source": [
        "**Plot Predictions**\\\n",
        "Run the cell below to compare your model's predictions to the true (ugly) function. Still not quite right? Try tweaking some of the hyperparameters above and re-run the cells in this section to see if you can improve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-86",
      "metadata": {
        "id": "cell-86"
      },
      "outputs": [],
      "source": [
        "# Plot predictions\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(x_ugly, y_ugly, alpha=0.6, lw=3, label='True Function')\n",
        "plt.plot(x_ugly, y_hat_ugly, label='NN Prediction', ls='--', c='r', lw=2)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Neural Network Approximation of Ugly Function')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-87",
      "metadata": {
        "id": "cell-87"
      },
      "source": [
        "Not bad!\n",
        "\n",
        "**End of team activity**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-89",
      "metadata": {
        "id": "cell-89"
      },
      "source": [
        "## Appendix - Multi-class Classification with PyTorch + Bagging\n",
        "\n",
        "So far we've only used our new PyTorch powers for toy regression and binary classification problems. Now we'll try classification with 3 classes!\n",
        "\n",
        "This example will use `seaborn`'s penguins dataset (last time, I promise!)\n",
        "\n",
        "We'll build a model to identify a penguin's species from its other features. In the process we'll dust off our Python skills with a *very* quick run through of a basic model building workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-92",
      "metadata": {
        "id": "cell-92"
      },
      "outputs": [],
      "source": [
        "# Bring on the penguins!\n",
        "penguins = pd.read_csv('data/penguins.csv')\n",
        "penguins.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-93",
      "metadata": {
        "id": "cell-93"
      },
      "source": [
        "We have 3 species of penguins living across 3 different islands. There are measurements of bill length, bill depth, flipper length, and body mass. We also have categorical variable for each penguin's sex giving us a total of 7 features.\n",
        "\n",
        "Here's a plot that tries to show too much at once. But you can ignore the marker shapes and sizes. The bill and flipper length alone ($x$ and $y$ axes) seem to already provide a fair amount of information about the species (color)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-94",
      "metadata": {
        "id": "cell-94"
      },
      "outputs": [],
      "source": [
        "# Plot penguins with too much info\n",
        "sns.relplot(data=penguins, x='flipper_length_mm', y='bill_length_mm',\n",
        "            hue='species', style='sex', size='body_mass_g', height=6);\n",
        "plt.title('Penguins!', fontdict={'color': 'teal', 'size': 20, 'weight': 'bold', 'family': 'serif'});"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-95",
      "metadata": {
        "id": "cell-95"
      },
      "source": [
        "We've taken care of *most* of the preprocessing in a separate notebook called `penguin_data.ipynb` and stored the results in `data/penguins_proc.csv`.\n",
        "\n",
        "Let's start from that file instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-96",
      "metadata": {
        "id": "cell-96"
      },
      "outputs": [],
      "source": [
        "penguins = pd.read_csv(\"data/penguins_proc.csv\")\n",
        "penguins.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-97",
      "metadata": {
        "id": "cell-97"
      },
      "outputs": [],
      "source": [
        "# Separate features from response\n",
        "X_design = penguins.drop(\"species\", axis=1)\n",
        "y = penguins.species"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-98",
      "metadata": {
        "id": "cell-98"
      },
      "source": [
        "But there are a few things we still need to do.\n",
        "\n",
        "**Feature Scaling**\n",
        "\n",
        "We should take a closer look at the range of values our predictors take on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-100",
      "metadata": {
        "id": "cell-100"
      },
      "outputs": [],
      "source": [
        "#  Summary stats of predictors\n",
        "X_design.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-101",
      "metadata": {
        "id": "cell-101"
      },
      "source": [
        "Our features are not on the same scale. Just compare the min/max of `bill_depth_mm` and `body_mass_g` for example.\\\n",
        "This can slow down neural network training for reasons we'll see in an upcoming lecture.\n",
        "\n",
        "Let's make use of `sklearn`'s `StandardScaler` to standardize the data, centering each predictor at 0 and setting their standard deviations to 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-103",
      "metadata": {
        "id": "cell-103"
      },
      "outputs": [],
      "source": [
        "# Remember the column names for later; we'll lose them when we scale\n",
        "X_cols = X_design.columns\n",
        "# Saving the scaler object in a variable allows us to reverse the transformation later\n",
        "scaler = StandardScaler()\n",
        "num_cols = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
        "X_scaled = X_design.copy()\n",
        "X_scaled[num_cols] = scaler.fit_transform(X_design[num_cols])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-104",
      "metadata": {
        "id": "cell-104"
      },
      "outputs": [],
      "source": [
        "# The scaler was passed a pandas DataFrame but returns a numpy array\n",
        "type(X_scaled), X_scaled.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-105",
      "metadata": {
        "id": "cell-105"
      },
      "outputs": [],
      "source": [
        "# We can always add the column names back later if we need to\n",
        "pd.DataFrame(X_scaled, columns=X_cols).head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-106",
      "metadata": {
        "id": "cell-106"
      },
      "source": [
        "**Encoding the Response Variable**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-107",
      "metadata": {
        "id": "cell-107"
      },
      "outputs": [],
      "source": [
        "# Take a look at our response\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-108",
      "metadata": {
        "id": "cell-108"
      },
      "source": [
        "Our response variable is still a `string`. We need to turn it into some numerical representation for our neural network.\\\n",
        "We could to this ourselves with a few list comprehensions, but `sklearn`'s `LabelEncoder` makes this very easy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-110",
      "metadata": {
        "id": "cell-110"
      },
      "outputs": [],
      "source": [
        "# Encode string labels as integers\n",
        "# LabelEncoder uses the familiar fit/transform methods we saw with StandardScaler\n",
        "labenc = LabelEncoder().fit(y)\n",
        "y_enc = labenc.transform(y)\n",
        "y_enc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-111",
      "metadata": {
        "id": "cell-111"
      },
      "outputs": [],
      "source": [
        "# We can recover the class labels from the encoder object later\n",
        "labenc.classes_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-112",
      "metadata": {
        "id": "cell-112"
      },
      "source": [
        "In PyTorch, for multi-class classification with `CrossEntropyLoss`, we can use integer labels directly (no one-hot encoding needed). The loss function expects:\n",
        "- Predictions: raw logits of shape (N, C) where C is number of classes\n",
        "- Targets: class indices of shape (N,) as integers\n",
        "\n",
        "We'll use integer labels with CrossEntropyLoss (more efficient in PyTorch)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c4od3dut1a",
      "metadata": {
        "id": "7c4od3dut1a"
      },
      "source": [
        "> **‚ùìQuestion 5: Predict & Interpret Multi-class Output**\n",
        ">\n",
        "> Suppose a trained penguin classifier outputs raw logits `[2.0, 0.5, -1.0]` for a single observation, where index 0=Adelie, 1=Chinstrap, 2=Gentoo.\n",
        ">\n",
        "> 1. Which species will be predicted (before applying softmax)?\n",
        "> 2. After applying softmax, the probabilities become approximately `[0.78, 0.17, 0.04]`. What does this tell us about the model's confidence?\n",
        "> 3. Why does PyTorch's `CrossEntropyLoss` expect raw logits instead of softmax probabilities?\n",
        "\n",
        "<details>\n",
        "<summary><em>Click to reveal answer</em></summary>\n",
        "\n",
        "> 1. Adelie (index 0) will be predicted since it has the highest logit value (2.0).\n",
        "> 2. The model is fairly confident (~78%) about Adelie, somewhat considers Chinstrap (~17%), and is quite sure it's not Gentoo (~4%).\n",
        "> 3. CrossEntropyLoss applies log-softmax internally for numerical stability. Computing softmax then log separately can cause numerical underflow/overflow issues.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-115",
      "metadata": {
        "id": "cell-115"
      },
      "source": [
        "\n",
        "**Train-test Split**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-118",
      "metadata": {
        "id": "cell-118"
      },
      "source": [
        "You may be familiar with using `train_test_split` to split the `X` and `y` arrays themselves. But here we will using it to create a set of train and test *indices*.\n",
        "\n",
        "We'll see later that being able to determine which rows in the original `X` and `y` ended up in train or test will be helpful.\n",
        "\n",
        "**Q:** But couldn't we just sample integers to get random indices? Why use `train_test_split`?\n",
        "\n",
        "**A:** Because `train_test_split` allows for **stratified** splitting!\n",
        "\n",
        "We'll stratify on `sex_Male`. By stratifying on this column we help ensure that proportion of male penguins is roughly equal in both train and test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-119",
      "metadata": {
        "id": "cell-119"
      },
      "outputs": [],
      "source": [
        "# Create train/test indices\n",
        "train_idx, test_idx = train_test_split(np.arange(X_scaled.shape[0]),\n",
        "                                                  test_size=0.5,\n",
        "                                                  random_state=109,\n",
        "                                                  stratify=X_scaled['sex_Male'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-120",
      "metadata": {
        "id": "cell-120"
      },
      "outputs": [],
      "source": [
        "# Index into X_scaled and y_enc to create the train and test sets\n",
        "X_train = X_scaled.iloc[train_idx].values  # Convert to numpy for PyTorch\n",
        "y_train = y_enc[train_idx]\n",
        "X_test = X_scaled.iloc[test_idx].values\n",
        "y_test = y_enc[test_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-121",
      "metadata": {
        "id": "cell-121"
      },
      "outputs": [],
      "source": [
        "# Sanity check on the resulting shapes\n",
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-122",
      "metadata": {
        "id": "cell-122"
      },
      "source": [
        "**Validation Split**\n",
        "\n",
        "We can also stratify for our validation split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-124",
      "metadata": {
        "id": "cell-124"
      },
      "outputs": [],
      "source": [
        "# Create train and validation splits from original train split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
        "                                                    test_size=0.5,\n",
        "                                                    random_state=109,\n",
        "                                                    stratify=X_scaled.iloc[train_idx][\"sex_Male\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-125",
      "metadata": {
        "id": "cell-125"
      },
      "source": [
        "---\n",
        "\n",
        "### Building the Penguin Classifier\n",
        "\n",
        "Now let's put everything together and build, train, and evaluate a multi-class neural network classifier for our penguin species.\n",
        "\n",
        "---\n",
        "  \n",
        "  > üí° **Note:** We use these PyTorch modules: `torch`, `nn`, `F`, `optim`\n",
        "\n",
        "**Build**\n",
        "\n",
        "We'll construct a neural network penguin classifier. Since we're using PyTorch's `CrossEntropyLoss`, we don't need a softmax activation on the output layer‚Äîthe loss function applies it internally.\n",
        "\n",
        "We determine the input and output dimensions programmatically from the data rather than hard-coding them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-128",
      "metadata": {
        "id": "cell-128"
      },
      "outputs": [],
      "source": [
        "# Build the model and print summary\n",
        "input_size = X_train.shape[1]\n",
        "output_size = len(np.unique(y_train))\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(input_size, 8),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(8, 16),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(16, output_size)  # No softmax - CrossEntropyLoss handles it\n",
        ")\n",
        "\n",
        "print(model)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-129",
      "metadata": {
        "id": "cell-129"
      },
      "source": [
        "**Setup Optimizer and Loss**\n",
        "\n",
        "We'll use `SGD` as our optimizer with a learning rate of 0.1.\n",
        "For multi-class classification, `CrossEntropyLoss` is the appropriate loss function‚Äîit combines `LogSoftmax` and `NLLLoss` in a single class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-130",
      "metadata": {
        "id": "cell-130"
      },
      "outputs": [],
      "source": [
        "# Setup optimizer and loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-131",
      "metadata": {
        "id": "cell-131"
      },
      "source": [
        "**Fit**\n",
        "\n",
        "Next we write a training loop for the model. We'll train for 50 epochs and track both training and validation metrics throughout the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-132",
      "metadata": {
        "id": "cell-132"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "\n",
        "# Convert data to tensors (ensure float/int types)\n",
        "X_train_t = torch.FloatTensor(X_train.astype(np.float32))\n",
        "y_train_t = torch.LongTensor(y_train.astype(np.int64))\n",
        "X_val_t = torch.FloatTensor(X_val.astype(np.float32))\n",
        "y_val_t = torch.LongTensor(y_val.astype(np.int64))\n",
        "\n",
        "# Training parameters\n",
        "epochs = 50\n",
        "history = {'loss': [], 'acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_t)\n",
        "    loss = criterion(outputs, y_train_t)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    train_acc = (predicted == y_train_t).float().mean()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_val_t)\n",
        "        val_loss = criterion(val_outputs, y_val_t)\n",
        "        _, val_predicted = torch.max(val_outputs, 1)\n",
        "        val_acc = (val_predicted == y_val_t).float().mean()\n",
        "\n",
        "    # Store history\n",
        "    history['loss'].append(loss.item())\n",
        "    history['acc'].append(train_acc.item())\n",
        "    history['val_loss'].append(val_loss.item())\n",
        "    history['val_acc'].append(val_acc.item())\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f} - Acc: {train_acc.item():.4f} - Val Loss: {val_loss.item():.4f} - Val Acc: {val_acc.item():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-133",
      "metadata": {
        "id": "cell-133"
      },
      "source": [
        "**Plot**\n",
        "\n",
        "Let's visualize the loss and accuracy across training epochs. We include both **training** and **validation** scores to monitor for overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-134",
      "metadata": {
        "id": "cell-134"
      },
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Loss plot\n",
        "axs[0].plot(history['loss'], linewidth=2, label='Training')\n",
        "axs[0].plot(history['val_loss'], linewidth=2, label='Validation', alpha=0.7)\n",
        "axs[0].set_xlabel('Epoch')\n",
        "axs[0].set_ylabel('Loss')\n",
        "axs[0].set_title('Cross Entropy Loss')\n",
        "axs[0].legend()\n",
        "\n",
        "# Accuracy plot\n",
        "axs[1].plot(history['acc'], linewidth=2, label='Training')\n",
        "axs[1].plot(history['val_acc'], linewidth=2, label='Validation', alpha=0.7)\n",
        "axs[1].set_xlabel('Epoch')\n",
        "axs[1].set_ylabel('Accuracy')\n",
        "axs[1].set_title('Accuracy')\n",
        "axs[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-135",
      "metadata": {
        "id": "cell-135"
      },
      "source": [
        "### Evaluating the Model\n",
        "\n",
        "First, let's see how well we could to by simply predicting the majority class in the training data for all observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-137",
      "metadata": {
        "id": "cell-137"
      },
      "outputs": [],
      "source": [
        "# Calculate naive accuracy (majority class baseline)\n",
        "most_common_class = Counter(y_train).most_common(1)[0][1]\n",
        "naive_acc = most_common_class / len(y_train)\n",
        "print('Naive Accuracy:', naive_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-138",
      "metadata": {
        "id": "cell-138"
      },
      "outputs": [],
      "source": [
        "# Helper function to evaluate model\n",
        "def evaluate_model(model, X, y, criterion):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_t = torch.FloatTensor(X.astype(np.float32))\n",
        "        y_t = torch.LongTensor(y.astype(np.int64))\n",
        "        outputs = model(X_t)\n",
        "        loss = criterion(outputs, y_t)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        accuracy = (predicted == y_t).float().mean()\n",
        "    return loss.item(), accuracy.item()\n",
        "\n",
        "print('Train:', evaluate_model(model, X_train, y_train, criterion))\n",
        "print('Validation:', evaluate_model(model, X_val, y_val, criterion))\n",
        "print('Test:', evaluate_model(model, X_test, y_test, criterion))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-139",
      "metadata": {
        "id": "cell-139"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-170",
      "metadata": {
        "id": "cell-170"
      },
      "source": [
        "### Bagging\n",
        "You'll be using bagging (\"bootstrap aggregating\") in your HW so let's take a minute to review the idea and see how it would work with a PyTorch model.\n",
        "    \n",
        "The idea is to simulate multiple datasets by sampling our current one with replacement and fitting a model on this sample. The process is repeated multiple times until we have an *ensemble* of fitted models, all trained on slightly different datasets.\n",
        "    \n",
        "We can then treat the ensemble as a single 'bagged' model. When it is time to predict, each model in the ensemble makes its own predictions. These predictions can then be *aggregated* across models, for example, by taking the average or through majority voting.\n",
        "    \n",
        "We may also be interested in looking at the distribution of the predictions for a given observation as this may help us quantify our uncertainty in a way in which we could not with a single model's predictions (even if that model outputs a probability!)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-171",
      "metadata": {
        "id": "cell-171"
      },
      "source": [
        "**NN Build Function**\n",
        "\n",
        "**Arguments:**\n",
        "- `name`: str - A name for your NN.\n",
        "- `input_dim`: int - number of predictors in input\n",
        "- `hidden_dims`: list of int - specifies the number of neurons in each hidden layer\n",
        "    - Ex: [2,4,8] would mean 3 hidden layers with 2, 4, and 8 neurons respectively\n",
        "- `hidden_act`: activation function used by all hidden layers\n",
        "- `out_dim`: int - number of output neurons a.k.a 'output units'\n",
        "- `out_act`: activation function used by output layer (or None)\n",
        "\n",
        "**Hint:** We will reuse this function throughout the notebook in different settings, but you should go ahead and set some sensible defaults for *all* of the arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-172",
      "metadata": {
        "id": "cell-172"
      },
      "outputs": [],
      "source": [
        "def build_NN(name='NN', input_dim=1, hidden_dims=[2], hidden_act=nn.ReLU, out_dim=1, out_act=None):\n",
        "    \"\"\"Build a neural network with configurable architecture\n",
        "    \"\"\"\n",
        "    layers = []\n",
        "    prev_dim = input_dim\n",
        "\n",
        "    for hidden_dim in hidden_dims:\n",
        "        layers.append(nn.Linear(prev_dim, hidden_dim))\n",
        "        layers.append(hidden_act())\n",
        "        prev_dim = hidden_dim\n",
        "\n",
        "    layers.append(nn.Linear(prev_dim, out_dim))\n",
        "    if out_act is not None:\n",
        "        layers.append(out_act())\n",
        "\n",
        "    return nn.Sequential(*layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-173",
      "metadata": {
        "id": "cell-173"
      },
      "outputs": [],
      "source": [
        "# Set up parameters for the bagging process\n",
        "learning_rate = 1e-1\n",
        "epochs = 50\n",
        "n_boot = 30\n",
        "bagged_models = []\n",
        "np.random.seed(109)\n",
        "torch.manual_seed(109)\n",
        "\n",
        "for n in range(n_boot):\n",
        "    # Bootstrap\n",
        "    boot_idx = np.random.choice(X_train.shape[0], size=X_train.shape[0], replace=True)\n",
        "    X_train_boot = X_train[boot_idx]\n",
        "    y_train_boot = y_train[boot_idx]\n",
        "\n",
        "    # Convert to tensors, ensuring X_train_boot is float32\n",
        "    X_boot_tensor = torch.FloatTensor(X_train_boot.astype(np.float32))\n",
        "    y_boot_tensor = torch.LongTensor(y_train_boot)\n",
        "\n",
        "    # Build\n",
        "    boot_model = build_NN(name=f'penguins_{n}',\n",
        "                         input_dim=X_train_boot.shape[1],\n",
        "                         hidden_dims=[8, 16, 32],\n",
        "                         hidden_act=nn.ReLU,\n",
        "                         out_dim=3,\n",
        "                         out_act=None)  # No softmax, CrossEntropyLoss handles it\n",
        "\n",
        "    # Setup optimizer and loss\n",
        "    optimizer = optim.SGD(boot_model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    boot_model.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = boot_model(X_boot_tensor)\n",
        "        loss = criterion(outputs, y_boot_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Store bootstrapped model\n",
        "    bagged_models.append(boot_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-174",
      "metadata": {
        "id": "cell-174"
      },
      "outputs": [],
      "source": [
        "# Notice we can programatically recover the shape of a model's output layer\n",
        "m = bagged_models[0]\n",
        "# Get the last Linear layer\n",
        "for layer in reversed(list(m.modules())):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        out_dim = layer.out_features\n",
        "        break\n",
        "print(out_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-175",
      "metadata": {
        "id": "cell-175"
      },
      "outputs": [],
      "source": [
        "def get_bagged_pred(bagged_models, X):\n",
        "    # Number of observations\n",
        "    n_obs = X.shape[0]\n",
        "    # Number of models in the bagged ensemble\n",
        "    n_models = len(bagged_models)\n",
        "    # Get prediction dimensions from first model\n",
        "    for layer in reversed(list(bagged_models[0].modules())):\n",
        "        if isinstance(layer, nn.Linear):\n",
        "            pred_dim = layer.out_features\n",
        "            break\n",
        "\n",
        "    # 3D tensor to store predictions from each bootstrapped model\n",
        "    # n_observations x n_classes x n_models\n",
        "    boot_preds = np.zeros((n_obs, pred_dim, n_models))\n",
        "\n",
        "    # Store all predictions in the tensor\n",
        "    X_tensor = torch.FloatTensor(X.astype(np.float32)) # Convert to float32 here\n",
        "    for i, model in enumerate(bagged_models):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = F.softmax(model(X_tensor), dim=1)\n",
        "            boot_preds[:,:,i] = probs.numpy()\n",
        "\n",
        "    # Average the predictions across models\n",
        "    bag_pred = boot_preds.mean(axis=-1)\n",
        "    return bag_pred, boot_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-176",
      "metadata": {
        "id": "cell-176"
      },
      "outputs": [],
      "source": [
        "# Get aggregated and unaggregated ensemble predictions\n",
        "bag_pred, boot_preds = get_bagged_pred(bagged_models, X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-177",
      "metadata": {
        "id": "cell-177"
      },
      "outputs": [],
      "source": [
        "# Example of aggregated predictions\n",
        "bag_pred[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-178",
      "metadata": {
        "id": "cell-178"
      },
      "outputs": [],
      "source": [
        "# Shape of unaggregated ensemble predictions tensor\n",
        "boot_preds.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-179",
      "metadata": {
        "id": "cell-179"
      },
      "outputs": [],
      "source": [
        "# Calculate bagged accuracy\n",
        "bag_acc = sum(bag_pred.argmax(axis=-1) == y_test)/bag_pred.shape[0]\n",
        "print(f'Single Model Test Acc: {evaluate_model(model, X_test, y_test, criterion)[1]:.4f}')\n",
        "print(f'Bagged Acc: {bag_acc:.4f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}